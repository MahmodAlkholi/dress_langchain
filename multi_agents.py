import streamlit as st
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import csv_loader , DirectoryLoader
import openai
from langsmith import traceable
from langsmith.wrappers import wrap_openai

import os
from dotenv import load_dotenv
load_dotenv()

os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["LANGCHAIN_API_KEY"] = os.getenv("LANGCHAIN_API_KEY")
os.environ["LANGCHAIN_TRACING_V2"] = "True"
os.environ["LANGCHAIN_PROJECT"] = os.getenv("LANGCHAIN_PROJECT")
grok_api_key = os.getenv("GROQ_API_KEY")
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    raise ValueError("OPENAI_API_KEY is not set in the environment or .env file.")
os.environ["OPENAI_API_KEY"] = openai_api_key

path = os.path.abspath("myntra_products_catalog.csv")



# loader = csv_loader.CSVLoader(path)
# docs = loader.load()


# text_splitter = RecursiveCharacterTextSplitter(chunk_size=4500, chunk_overlap=200)
# splits = text_splitter.split_documents(docs)

# # Embed and store in Chroma
# vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings() , persist_directory="./chroma")

# # Index
# retriever = vectorstore.as_retriever()

vectorstore = Chroma(persist_directory="./temp", embedding_function = OpenAIEmbeddings() )
retriever = vectorstore.as_retriever()
### RAG bot



class RagBot:

    def __init__(self, retriever, model: str = "gpt-4o-mini"):
        self._retriever = retriever
        # Wrapping the client instruments the LLM
        self._client = wrap_openai(openai.Client())
        self._model = model

    @traceable()
    def retrieve_docs(self, question):
        return self._retriever.invoke(question)

    @traceable()
    def invoke_llm(self, question, docs):
        response = self._client.chat.completions.create(
            model=self._model,
            messages=[
                {
                    "role": "system",
                    "content": """

                            - Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ ÙÙŠ Ù…ØªØ¬Ø± Ø§Ù„Ù…Ù„Ø§Ø¨Ø³ Ø§Ù„Ø¬Ø§Ù‡Ø²Ø©. ÙŠÙ…ÙƒÙ†Ùƒ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ:

                            - ØªÙ‚Ø¯ÙŠÙ… Ù†ØµØ§Ø¦Ø­ Ù„Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…Ù‚Ø§Ø³Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©.
                             - Ø§Ù‚ØªØ±Ø§Ø­ ØªÙ†Ø³ÙŠÙ‚Ø§Øª Ù…Ù„Ø§Ø¨Ø³ ØªÙ†Ø§Ø³Ø¨ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ø£Ùˆ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ø´Ø®ØµÙŠ.
                            - Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª.

                                """
                    "  Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© Ù„ØªÙ‚Ø¯ÙŠÙ… Ø§Ø¬Ø§Ø¨Ø© Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø§Ù…ÙŠØ© Ø§Ù„Ù…ØµØ±ÙŠØ©.\n\n"
                    f"## Docs\n\n{docs}",
                },
                {"role": "user", "content": question},
            ],
        )

        # Evaluators will expect "answer" and "contexts"
        return {
            "answer": response.choices[0].message.content,
            "contexts": [str(doc) for doc in docs],
        }

    @traceable()
    def get_answer(self, question: str):
        docs = self.retrieve_docs(question)
        return self.invoke_llm(question, docs)

rag_bot = RagBot(retriever)



st.set_page_config(page_icon= "ğŸ›ï¸" , layout="wide")

st.header("ğŸ›ï¸ Ù…Ø±Ø­Ø¨Ø§ Ø¨ÙƒÙ… ÙÙŠ Ù…Ø­Ù„Ø§Øª Ø§Ù„Ù…Ù„Ø§Ø¨Ø³")
st.subheader("Ø§Ù†Ø§ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø´Ø®ØµÙŠ ")

question = st.text_area("Ø¨Ù…Ø§ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø§Ù† Ø§Ø³Ø§Ø¹Ø¯Ùƒ" , height=250)
btn = st.button("SUBMET")
if question:
    if btn:
        with st.spinner('Wait ...'):
   

            response = rag_bot.get_answer(question)
            st.write(response["answer"])



